# -*- coding: utf-8 -*-
"""Examen Aprendizaje Inteligente Parcial II.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17vDKQw88e5kbPmBr4EfJocYY6gtmJGqL

<img  src="https://www.uaa.mx/portal/wp-content/uploads/2022/08/UAA-LOGO.png">

# UNIVERSIDAD AUTÓNOMA DE AGUASCALIENTES

# CENTRO DE CIENCIAS BASICAS

# DEPARTAMENTO DE CIENCIAS DE LA COMPUTACIÓN

# APRENDIZAJE INTELIGENTE

# INGENIERÍA EN COMPUTACIÓN INTELIGENTE

# Examen Aprendizaje Inteligente Parcial II

#Profesor: Francisco Javier Luna Rosas

# César Omar Alatorre López 

# César Arturo Montoya Esqueda 

# Cristian Israel Donato Flores 

# Emilio Luna Pérez

# Gabriel Melchor Campos

# Semestre Enero-Junio 2023

El análisis de sentimientos, a veces también denominado minería de opiniones, es una conocida 
sub-disciplina del amplio campo del PLN (Procesamiento del Lenguaje Natural); está relacionado 
con el análisis de la polaridad de documentos. Una tarea popular en el análisis de sentimiento es 
la clasificación de documentos basados en las emociones u opiniones expresadas de los autores 
respecto a un tema en particular. El conjunto de datos de críticas de cine consiste en 50000 
críticas de cine polarizadas etiquetadas como negativas y como positivas. Aquí, positiva significa 
que una película ha sido clasificada con más de seis estrellas, mientras que negativa significa que 
una película ha sido clasificada con menos de cinco estrellas.

a) Una explicación del Pre-procesamiento de datos para generar un formato adecuado de los datos.

Lectura el dataset
"""

#Importamos pandas
import pandas as pd
#Leemos el dataset
df = pd.read_csv('movie_data.csv')
#Mostramos las primeras filas
df.head(10)

"""Limpieza de datos"""

#Funcion auxiliar de limpieza
import re
def Limpiador(text):
    # Remover tags de html
    text = re.sub('<[^<]*>','',text)
    
    # Almacenar temporalmente los emoticons
    emoticons = ''.join(re.findall('[:;=]-+[\)\(pPD]+',text))
    
    # Elimine los caracteres que no son palabras y combinar los emoticones
    text = re.sub('\W+',' ',text.lower()) + emoticons.replace('-','')
    
    return text

#Funcion para remover emojis
def RemoverEmoji(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

#Aplicamos los limpiadores
df['review'] = df['review'].apply(Limpiador)

#Remover Emojis
df['review']=df['review'].apply(lambda x: RemoverEmoji(x))

#Normalizar a utf-8, remover acentos
from unicodedata import normalize

RemoverAcentos = lambda text: normalize("NFKD", text).encode("ascii", "ignore").decode("utf-8", "ignore")

df["review"] = df["review"].apply(RemoverAcentos)

#Mostramos datos limpios
df.head(10)

"""b) Una explicación del modelo bolsa de palabras o cualquier otro analizador liguistico aplicado al 
Dataset de críticas de cine.

Descargamos paquetes auxiliares del toolkit de procesamiento de lenguaje natural

Hemos descargado la lista de palabras vacías (stopwords) de nltk, que es una biblioteca de procesamiento de lenguaje natural en Python.
"""

#importamos el toolkit de procesamiento de lenguaje natural
import nltk
nltk.download('stopwords')
import nltk
nltk.download('punkt')

"""
Las palabras vacías son palabras comunes que a menudo se eliminan de los textos durante el análisis de texto porque no aportan mucho significado en sí mismas. Ejemplos de palabras vacías en inglés incluyen "a", "an", "the", "in", "on", "of", etc."""

#Importamos stopwords
from nltk.corpus import stopwords
#Seleccionamos el lenguaje
stop = stopwords.words('english')

#Importamos el tokenizador de palabras
from nltk import word_tokenize
#Seleccioanmos idioma
stop_words = set(stopwords.words('english'))
#Seleccionamos la columna de review
corpora = df["review"].values
#Tokenizamos todas las entradas del dataset en review
tokenized = [word_tokenize(corpus) for corpus in corpora]

#Eliminamos los stopwords de todas las entradas del dataset que ya ha sido tokenizado
tokens_sin_stopwords = [[word for word in sublist if word.lower() not in stop_words] for sublist in tokenized]

#Eliminamos las palabras de tamaño 1
tokenized_final = [list(filter(lambda x: len(x) > 1, document)) \
             for document in tokens_sin_stopwords]

#Mostramos un ejemplo tokenizado y sin stopwords
print(tokenized_final[2222])

"""c) Una explicación de la transformación de las palabras en vectores de características (utilice la 
frecuencia de termino - frecuencia inversa de documento “tf-idf” o cualquier otra técnica que 
permita transformar palabras a vectores de características).
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #Importamos la libreria de conversion de palabras a vectores
# from gensim.models import word2vec
# #Importamos numpy
# import numpy as np
# #Suprimimos las salidas
# np.set_printoptions(suppress=True)
# #Caracteristicas para convertir a vectores
# 
# #Genracion de 513 columnas para entrenar
# feature_size = 513
# #Tamaño del contexto de palabras
# context_size = 8
# #Palabra minima para vectorizar
# min_word = 1
# 
# word_vec= word2vec.Word2Vec(tokenized_final, vector_size=feature_size, \
#                             window=context_size, min_count=min_word, \
#                             epochs=100, seed=42)

"""Este código extrae los vectores de palabras de un modelo entrenado de Word2Vec y crea un dataframe de Pandas que contiene cada palabra como índice y sus correspondientes vectores como valores.

La primera línea crea una lista de tuplas donde cada tupla contiene una palabra y su índice correspondiente en el modelo entrenado de Word2Vec. La segunda línea utiliza la función zip() de Python para separar las palabras y los índices en dos tuplas diferentes. La tercera línea crea un dataframe de Pandas utilizando los índices como índice de filas y los vectores correspondientes como valores en cada columna. Por último, la función head() se utiliza para mostrar las primeras filas del dataframe.
"""

# crea una lista de tuplas donde cada tupla contiene una palabra y su índice correspondiente en el modelo entrenado de Word2Vec
word_vec_unpack = [(word, idx) for word, idx in \
                   word_vec.wv.key_to_index.items()]

#segunda línea utiliza la función zip() de Python para separar las palabras y los índices en dos tuplas diferentes
tokens, indexes = zip(*word_vec_unpack)

#crea un dataframe de Pandas utilizando los índices como índice de filas y los vectores correspondientes como valores en cada columna
word_vec_df = pd.DataFrame(word_vec.wv.vectors[indexes, :], index=tokens)

#la función head() se utiliza para mostrar las primeras filas del dataframe
word_vec_df.head()

# crea una matriz NumPy a partir de los tokens previamente procesados
tokenized_array = np.array(tokenized_final)
#se crea una matriz de modelo para cada documento en el corpus procesado, promediando los vectores de cada palabra en el documento utilizando la matriz previamente creada
model_array = np.array([word_vec_df.loc[doc].mean(axis=0) for doc in tokenized_array])

#Creamos un nuevo dataframe
model_df = pd.DataFrame(model_array)
#Agregamos la columna de sentimiento
model_df["sentiment"] = df["sentiment"]

model_df.head()

#Guardamos el dataframe para poder entrenar como un nuevo dataset
model_df.to_csv('procesed_movie_data2.csv', encoding='utf-8',index=False)

#Asignamos la variable de clases
y=model_df["sentiment"]
y

#Eliminamos la columna y dejamos la variables para predecir
del model_df["sentiment"]
x=model_df

"""Elegimos utilizar las técnicas TF-IDF para los clasificadores de Naive Bayes y SVM porque, en conjunto, mejoran notablemente la precisión en la clasificación del texto. """

#Libreria de procesamiento de lenguaje natural
import nltk
#Importamos stopwords
nltk.download('stopwords')
#Seleccionamos el lenguaje
stopw = stopwords.words('english')

#stemming para eliminar derivaciones de palabras comunes a una palabra base como organizar, organizado, organizador, organización
# Derivación corta palabras y elimina derivaciones
from nltk.stem.porter import PorterStemmer

porter = PorterStemmer()

"""La función tokenizer_porter toma una cadena de texto como entrada y la divide en palabras individuales utilizando el método split(). Luego, utiliza la clase PorterStemmer para aplicar el algoritmo de stemming de Porter a cada palabra y devolver una lista de las palabras reducidas a su forma base."""

#tokenize el texto y divida la oración en palabras según la ocurrencia
#técnica steamming
def tokenizer(text):
    return text.split()
def tokenizer_porter(text):
    return[porter.stem(word) for word in text.split()]

"""TfidfVectorizer de la biblioteca sklearn.feature_extraction.text para construir una matriz TF-IDF """

from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords

tfidf = TfidfVectorizer(strip_accents = None,
                        lowercase = False,
                        preprocessor = None, #because we already did on our data
                        tokenizer = tokenizer_porter, #stemming function
                        use_idf = True,    #to downgrade according to frequency
                        norm = 'l2',
                        stop_words=stopw,
                        smooth_idf = True   #to avoid division by zero
                       )

#split to x, y components
Y_ = df.sentiment.values   #numpy array
X_ = tfidf.fit_transform(df.review)

from sklearn.model_selection import train_test_split
#Segmentamos las variables para evaluar y entrnar
X_train, X_test, Y_train, Y_test = train_test_split(X_,Y_,
                                                    random_state = 42, #to get same as instructor
                                                    test_size = 0.2
                                                   )

"""d) Una explicación de los modelos de Machine Learning utilizando: KNN, Naive Bayes, SVM, Redes 
Neuronal, Arboles de Decisión, Random Forest y Métodos de Potenciación para clasificar las
críticas de cine (el modelo de entrenamiento puede ser tabla testing, LOOCV, k-folds, etc.). La 
presión del modelo debe ser del 95% o mayor.

Redes neuronales
"""

#redes neuronales
from sklearn.neural_network import MLPClassifier
clf = MLPClassifier(solver='adam', alpha=1e-5,
                    hidden_layer_sizes=(58, 20,30,5,2), random_state=1)

clf.fit(X,y)
clf.score(X,y)

# Create confusion matrix para redes neuronales
from sklearn.metrics import confusion_matrix
prediccion_nn = clf.predict(x)
MC_nn = confusion_matrix(y,prediccion_nn)

presicion_global_nn = np.sum(MC_nn.diagonal())/np.sum(MC_nn)
error_global_nn = 1 - presicion_global_nn

precision_categoria_nn = pd.DataFrame(MC_nn.diagonal()/np.sum(MC_nn,axis=1)).T

print ("Matriz de confusión: \n",MC_nn)
print ("Precisión global: \n",presicion_global_nn)
print ("Error global: \n",error_global_nn)
print ("Precisión por catergoria: \n",precision_categoria_nn )

"""KNN"""

#knn
from sklearn.neighbors import KNeighborsClassifier
#Generamos la instancia
knn = KNeighborsClassifier(n_neighbors=22, weights='distance')
#Entrenamos usando tabla completa
knn.fit(x, y)
#Evaluamos usando todas las entradas
knn.score(x, y)

# Create confusion matrix para knn
from sklearn.metrics import confusion_matrix
prediccion = knn.predict(x)
MC = confusion_matrix(y,prediccion)

presicion_global = np.sum(MC.diagonal())/np.sum(MC)
error_global = 1 - presicion_global

precision_categoria = pd.DataFrame(MC.diagonal()/np.sum(MC,axis=1)).T

print ("Matriz de confusión: \n",MC)
print ("Precisión global: \n",presicion_global)
print ("Error global: \n",error_global)
print ("Precisión por catergoria: \n",precision_categoria )

"""Arboles de desicion"""

#Arboles de desicion
from sklearn import tree
#Generamos la instacia
desiciontree = tree.DecisionTreeClassifier()
#Entrenamos usando tabla completa
desiciontree.fit(x,y)
#Evaluamos usando todas las entradas
desiciontree.score(x,y)

# Create confusion matrix para arboles de desición
from sklearn.metrics import confusion_matrix
prediccion_dt = desiciontree.predict(x)
MC_desiciontree = confusion_matrix(y,prediccion_dt)

presicion_global_dt = np.sum(MC_desiciontree.diagonal())/np.sum(MC_desiciontree)
error_global_dt = 1 - presicion_global_dt

precision_categoria_dt = pd.DataFrame(MC_desiciontree.diagonal()/np.sum(MC_desiciontree,axis=1)).T

print ("Matriz de confusión: \n",MC_desiciontree)
print ("Precisión global: \n",presicion_global_dt)
print ("Error global: \n",error_global_dt)
print ("Precisión por catergoria: \n",precision_categoria_dt )

"""Bosques aleatorios"""

#Random Forest
from sklearn.ensemble import RandomForestClassifier
#Generamos la instancia
randomforest = RandomForestClassifier()
#Entrenamos usando tabla completa
randomforest.fit(x,y)
#Evaluamos usando todas las entradas
randomforest.score(x,y)

# Create confusion matrix para bosques aleatorios
from sklearn.metrics import confusion_matrix
prediccion_randomforest = randomforest.predict(x)
MC_randomforest = confusion_matrix(y,prediccion_randomforest)

presicion_global_rf = np.sum(MC_randomforest.diagonal())/np.sum(MC_randomforest)
error_global_rd = 1 - presicion_global_rf

precision_categoria_rf = pd.DataFrame(MC_randomforest.diagonal()/np.sum(MC_randomforest,axis=1)).T

print ("Matriz de confusión: \n",MC_randomforest)
print ("Precisión global: \n",presicion_global_rf)
print ("Error global: \n",error_global_rd)
print ("Precisión por catergoria: \n",precision_categoria_rf )

"""Adaboost Clasificador"""

from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier
#Generamos la instancia
adaboostclasifier = AdaBoostClassifier(n_estimators=100,base_estimator=RandomForestClassifier())
#Entrenamos usando tabla completa
adaboostclasifier.fit(x, y)
#Evaluamos usando todas las entradas
adaboostclasifier.score(x,y)

# Create confusion matrix para Adaboost Clasificador
from sklearn.metrics import confusion_matrix
prediccion_adaboostclasifier = adaboostclasifier.predict(x)
MC_adaboost = confusion_matrix(y,prediccion_adaboostclasifier)

presicion_global_ab = np.sum(MC_adaboost.diagonal())/np.sum(MC_adaboost)
error_global_ab = 1 - presicion_global_ab

precision_categoria_ab = pd.DataFrame(MC_adaboost.diagonal()/np.sum(MC_adaboost,axis=1)).T

print ("Matriz de confusión: \n",MC_adaboost)
print ("Presición global: \n",presicion_global_ab)
print ("Error global: \n",error_global_ab)
print ("Presición por catergoria: \n",precision_categoria_ab )

"""XGBoost Clasificador"""

# Importaciones adicionales
from xgboost import XGBClassifier

#Generamos la instancia
xgb_classifier = XGBClassifier(
    use_label_encoder=False,
    eval_metric='auc',
    learning_rate=0.1,
    n_estimators=100,
    max_depth=10,
    min_child_weight=1,
    gamma=0,
    booster='gbtree',
    subsample=0.9,
    colsample_bytree=0.9,
    scale_pos_weight=1
)
# Entrenar el clasificador XGBoost
xgb_classifier.fit(x, y)
#Evaluamos usando todas las entradas
xgb_classifier.score(x, y)

# Create confusion matrix para xgboost
from sklearn.metrics import confusion_matrix
prediccion_xgboost = xgb_classifier.predict(x)
MC_xgboost = confusion_matrix(y,prediccion_xgboost)

presicion_global_xgboost = np.sum(MC_xgboost.diagonal())/np.sum(MC_xgboost)
error_global_xgboost = 1 - presicion_global_xgboost

precision_categoria_xgboost = pd.DataFrame(MC_xgboost.diagonal()/np.sum(MC_xgboost,axis=1)).T

print ("Matriz de confusión: \n",MC_xgboost)
print ("Presición global: \n",presicion_global_xgboost)
print ("Error global: \n",error_global_xgboost)
print ("Presición por catergoria: \n",precision_categoria_xgboost )

#Maquinas de soporte vectorial
from sklearn import svm

#Create a svm Classifier
maquinassoporte = svm.SVC() 

maquinassoporte.fit(X_,Y_)

maquinassoporte.score(X_test, Y_test)

# Create confusion matrix para Maquinas de soporte vectorial
from sklearn.metrics import confusion_matrix
prediccion_svm = maquinassoporte.predict(X_test)
MC_svm = confusion_matrix(Y_test,prediccion_svm)

presicion_global_svm = np.sum(MC_svm.diagonal())/np.sum(MC_svm)
error_global_svm = 1 - presicion_global_svm

precision_categoria_svm = pd.DataFrame(MC_svm.diagonal()/np.sum(MC_svm,axis=1)).T

print ("Matriz de confusión: \n",MC_svm)
print ("Presición global: \n",presicion_global_svm)
print ("Error global: \n",error_global_svm)
print ("Presición por catergoria: \n",precision_categoria_svm )

#Naive Bayes
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import MultinomialNB

pipe_mnb = make_pipeline(StandardScaler(with_mean=False), MultinomialNB())
pipe_mnb.fit(X_,Y_) # apply scaling on training data

pipe_mnb.score(X_test, Y_test)

# Create confusion matrix para Naive Bayes
from sklearn.metrics import confusion_matrix
import numpy as np

prediccion_bayes = pipe_mnb.predict(X_test)
MC_bayes = confusion_matrix(Y_test,prediccion_bayes)

presicion_global_bayes = np.sum(MC_bayes.diagonal())/np.sum(MC_bayes)
error_global_bayes = 1 - presicion_global_bayes

precision_categoria_bayes = pd.DataFrame(MC_bayes.diagonal()/np.sum(MC_bayes,axis=1)).T

print ("Matriz de confusión: \n",MC_bayes)
print ("Presición global: \n",presicion_global_bayes)
print ("Error global: \n",error_global_bayes)
print ("Presición por catergoria: \n",precision_categoria_bayes )

"""e) Una explicación del análisis comparativo de modelos de Machine Learning utilizando los clasificadores previamente mencionados, el análisis deberá comparar: la precisión del modelo, el error del modelo, precisión negativa especificidad), precisión positiva (sensibilidad), falsos positivos, falsos negativos, asertividad positiva, asertividad negativa."""

# Agregar los modelos al diccionario
models_dict = {
    "Arboles de desición": desiciontree,
    "Bosques Aleatorios": randomforest,
    "AdaBoostClassifier": adaboostclasifier,
    "XGB_Classifier": xgb_classifier,
    "Redes Neuronales":clf,
    "KNN":knn,
}

#   "SVM":maquinassoporte


# calcular métricas de evaluación
tn_svm, fp_svm, fn_svm, tp_svm = MC_svm.ravel()
sensitivity_svm = tp_svm / (tp_svm + fn_svm)
specificity_svm = tn_svm / (tn_svm + fp_svm)
fpr_svm = fp_svm / (fp_svm + tn_svm)
fnr_svm = fn_svm / (fn_svm + tp_svm)
ppv_svm = tp_svm / (tp_svm + fp_svm)
npv_svm = tn_svm / (tn_svm + fn_svm)

# mostrar informe de evaluación
print(f"Precisión positiva (sensibilidad): {sensitivity_svm}")
print(f"Precisión negativa (especificidad): {specificity_svm}")
print(f"Falsos positivos (tasa de falsos positivos): {fpr_svm}")
print(f"Falsos negativos (tasa de falsos negativos): {fnr_svm}")
print(f"Asertividad positiva: {ppv_svm}")
print(f"Asertividad negativa: {npv_svm}")

#   "Naive Bayes": pipe_mnb,

tn_bayes, fp_bayes, fn_bayes, tp_bayes = MC_bayes.ravel()
sensitivity_bayes = tp_bayes / (tp_bayes + fn_bayes)
specificity_bayes = tn_bayes / (tn_bayes + fp_bayes)
fpr_bayes = fp_bayes / (fp_bayes + tn_bayes)
fnr_bayes = fn_bayes / (fn_bayes + tp_bayes)
ppv_bayes = tp_bayes / (tp_bayes + fp_bayes)
npv_bayes = tn_bayes / (tn_bayes + fn_bayes)

# mostrar informe de evaluación
print(f"Precisión positiva (sensibilidad): {sensitivity_bayes}")
print(f"Precisión negativa (especificidad): {specificity_bayes}")
print(f"Falsos positivos (tasa de falsos positivos): {fpr_bayes}")
print(f"Falsos negativos (tasa de falsos negativos): {fnr_bayes}")
print(f"Asertividad positiva: {ppv_bayes}")
print(f"Asertividad negativa: {npv_bayes}")

import pandas as pd
from sklearn.metrics import confusion_matrix

# crear listas vacías para cada métrica de evaluación
models = []
accuracies = []
sensitivities = []
specificities = []
fprs = []
fnrs = []
ppvs = []
npvs = []

# iterar sobre cada modelo y calcular las métricas de evaluación
for model_name, model in models_dict.items():
    y_true = model.predict(X)
    y_pred = y
    conf_mat = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = conf_mat.ravel()
    sensitivity = tp / (tp + fn)
    specificity = tn / (tn + fp)
    fpr = fp / (fp + tn)
    fnr = fn / (fn + tp)
    ppv = tp / (tp + fp)
    npv = tn / (tn + fn)

    # agregar las métricas al dataframe
    models.append(model_name)
    accuracies.append(model.score(X, y))
    sensitivities.append(sensitivity)
    specificities.append(specificity)
    fprs.append(fpr)
    fnrs.append(fnr)
    ppvs.append(ppv)
    npvs.append(npv)

# crear el dataframe y mostrarlo en la consola
df = pd.DataFrame({
    "Model": models,
    "Accuracy": accuracies,
    "Sensitivity": sensitivities,
    "Specificity": specificities,
    "FPR": fprs,
    "FNR": fnrs,
    "PPV": ppvs,
    "NPV": npvs
})
print(df)

# agregar las métricas al dataframe
models.append("Bayes")
accuracies.append(0.9397)
sensitivities.append(sensitivity_bayes)
specificities.append(specificity_bayes)
fprs.append(fpr_bayes)
fnrs.append(fnr_bayes)
ppvs.append(ppv_bayes)
npvs.append(npv_bayes)

models.append("SVM")
accuracies.append(0.9869)
sensitivities.append(sensitivity_svm)
specificities.append(specificity_svm)
fprs.append(fpr_svm)
fnrs.append(fnr_svm)
ppvs.append(ppv_svm)
npvs.append(npv_svm)

# crear el dataframe y mostrarlo en la consola
df = pd.DataFrame({
    "Model": models,
    "Accuracy": accuracies,
    "Sensitivity": sensitivities,
    "Specificity": specificities,
    "FPR": fprs,
    "FNR": fnrs,
    "PPV": ppvs,
    "NPV": npvs
})
print(df)

"""f) Una explicación del análisis comparativo de modelos de Machine Learning utilizando los 
clasificadores previamente mencionados, el análisis deberá comparar los modelos utilizando la 
curva ROC.
"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, RocCurveDisplay

for name, clf in models_dict.items():
    y_pred_proba = clf.predict_proba(x)[:,1]
    fpr, tpr, thresholds = roc_curve(y, y_pred_proba)
    svc_disp = RocCurveDisplay.from_estimator(clf, x, y, name=name)
    svc_disp.plot()

plt.legend(loc="lower right")
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, RocCurveDisplay

colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k'] # list of colors to use for each curve
i = 0 # variable to keep track of current color index

for name, clasifi in models_dict.items():
  y_pred_proba = clasifi.predict_proba(X)[:,1]
  fpr, tpr, thresholds = roc_curve(y, y_pred_proba)
  plt.plot(fpr, tpr, color=colors[i], label=name) # plot the curve with the current color
  i += 1 # increment the color index for the next curve

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Datos proporcionados
data = {
    'Model': ['Arboles de desición', 'Bosques Aleatorios', 'AdaBoostClassifier', 'XGB_Classifier', 'Redes Neuronales', 'KNN', 'Bayes', 'SVM'],
    'Accuracy': [1.00000, 1.00000, 1.00000, 0.99998, 0.98752, 1.00000, 0.93970, 0.98690],
    'Sensitivity': [1.000000, 1.000000, 1.000000, 1.000000, 0.999549, 1.000000, 0.916834, 0.987314],
    'Specificity': [1.000000, 1.000000, 1.000000, 0.999960, 0.976057, 1.000000, 0.962257, 0.986492],
    'FPR': [0.000000, 0.000000, 0.000000, 0.000040, 0.023943, 0.000000, 0.037743, 0.013508],
    'FNR': [0.000000, 0.000000, 0.000000, 0.000000, 0.000451, 0.000000, 0.083166, 0.012686],
    'PPV': [1.000000, 1.000000, 1.000000, 0.999960, 0.975480, 1.000000, 0.959941, 0.986321],
    'NPV': [1.000000, 1.000000, 1.000000, 1.000000, 0.999560, 1.000000, 0.921438, 0.987473]
}

# Crear un DataFrame con los datos
df = pd.DataFrame(data)

# Crear la figura y el eje para el gráfico
fig, ax = plt.subplots()

# Trazar la línea diagonal (representa un clasificador aleatorio)
ax.plot([0, 1], [0, 1], linestyle='--', color='gray')

# Trazar los puntos de cada modelo en el espacio ROC
for index, row in df.iterrows():
    ax.plot(row['FPR'], row['Sensitivity'], marker='o', label=row['Model'])

# Configurar etiquetas de los ejes y título
ax.set_xlabel('Tasa de falsos positivos (FPR)')
ax.set_ylabel('Tasa de verdaderos positivos (Sensitivity)')
ax.set_title('Espacio ROC')

# Añadir leyenda
ax.legend()

# Mostrar el gráfico
plt.show()

"""Conclusiones

Después de trabajar en nuestro examen, nos dimos cuenta de que convertir los datos en vectores llevó bastante tiempo, pero era un paso necesario para poder aplicar los modelos de aprendizaje automático. Probamos modelos como SVM y Naive Bayes utilizando la técnica TF-IDF. A pesar de que su ejecución duró mucho, sus resultados no fueron tan buenos como esperábamos. 

Por eso, decidimos probar otras técnicas para mejorar los resultados y obtener una mejor precisión en la clasificación de nuestros datos. Gracias a las clases impartidas por nuestro profesor y sus explicaciones durante las sesiones, pudimos comprender mejor las distintas técnicas y enfoques para abordar el problema. 

Finalmente, con el conocimiento adquirido, logramos mejorar nuestros resultados y encontrar un enfoque que funcionara bien para nuestro examen. En general, este examen nos permitió aplicar lo aprendido en clase y obtener una valiosa experiencia práctica en la resolución de problemas de clasificación utilizando diferentes métodos de aprendizaje automático.

Bibliografía


Equipo de desarrollo de Pandas. (s. f.). Documentación de pandas. pandas. Recuperado de https://pandas.pydata.org/docs/ 

W3Schools. (s. f.). Python RegEx. Recuperado de https://www.w3schools.com/python/python_regex.asp 

GeeksforGeeks. (s. f.). Python - Derivación de palabras con NLTK. Recuperado de https://www.geeksforgeeks.org/python-stemming-words-with-nltk/ 

Desarrolladores de scikit-learn. (s. f.). sklearn.feature_extraction.text.TfidfVectorizer — Documentación de scikit-learn 1.0.1. scikit-learn. Recuperado de https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html 

Desarrolladores de scikit-learn. (s. f.). scikit-learn: aprendizaje automático en Python — Documentación de scikit-learn 1.0.1. scikit-learn. Recuperado de https://scikit-learn.org/stable/
"""